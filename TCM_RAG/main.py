# main.py
import os
import sys
import torch
from services.rag_service import EnterpriseRAGService
from utils.logger import logger
from config.settings import Settings


# è®¾ç½®LlamaIndexå…¨å±€æ—¥å¿—çº§åˆ« (å¯é€‰ï¼Œä¸è‡ªå®šä¹‰loggeré…åˆä½¿ç”¨)
# llama_index.core.set_global_handler("simple")

def main():
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)

    # 1. åŠ è½½é…ç½®ï¼ˆå¯ä»¥ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½ï¼‰
    os.environ["EMBED_MODEL_PATH"] = "/home/ubuntu/PycharmProjects/pythonRAG/models/BAAI/bge-large-zh"
    os.environ["LLM_MODEL_PATH"] = "/home/ubuntu/PycharmProjects/pythonRAG/models/qwen/Qwen2-7B-Instruct"
    os.environ["RERANK_MODEL_PATH"] = "/home/ubuntu/PycharmProjects/pythonRAG/models/BAAI/bge-reranker-base"

    # åˆå§‹åŒ–RAGæœåŠ¡
    rag_service = EnterpriseRAGService()

    # 2. åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
    try:
        rag_service.initialize()
    except Exception as e:
        logger.log_error(e, {"stage": "main_initialization"})
        print("âŒ RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œæ¨¡å‹è·¯å¾„ã€‚", file=sys.stderr)
        sys.exit(1)

    print("\n==")
    print("ğŸ¤– ä¼ä¸šçº§æ™ºèƒ½å¤šè·¯ RAG å·²å¯åŠ¨ â€”â€” è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("==")

    # 3. ä¸»å¾ªç¯
    while True:
        try:
            q = input("\nâ“ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
            if not q:
                continue
            if q.lower() in ["exit", "quit", "é€€å‡º"]:
                break

            response = rag_service.query(q, user_id="user_console_001", include_debug=True)

            if "error" in response:
                print(f"\nâŒ é”™è¯¯: {response['error']}")
                print(f"ğŸ’¡ ç­”æ¡ˆ: {response.get('answer', 'æ— æ³•æä¾›ç­”æ¡ˆã€‚')}")
            else:
                print(f"\nğŸ’¡ ç­”æ¡ˆ:\n{response['answer']}")
                print("\nğŸ“š å‚è€ƒèµ„æ–™æ¥æº:")
                if response['sources']:
                    for i, src in enumerate(response['sources'], 1):
                        print(f" {i}. æ–‡ä»¶: {src['file_name']} | åˆ†æ•°: {src['score']:.4f} | é¢„è§ˆ: {src['preview']}")
                else:
                    print(" æ— ")

                print(
                    f"\nâ± æ€»è€—æ—¶: {response['total_time']:.2f} ç§’ (æ£€ç´¢: {response['retrieval_time']:.2f}s, ç”Ÿæˆ: {response['generation_time']:.2f}s)"
                )
                print(f"ğŸ“Š ç½®ä¿¡åº¦: {response['confidence']:.2%}")
                print(f"âš¡ ç¼“å­˜å‘½ä¸­: {'æ˜¯' if response['cache_hit'] else 'å¦'} (å¬å›æ–¹å¼: {response['method_used']})")

                if 'debug' in response:
                    print("\n--- è°ƒè¯•ä¿¡æ¯ ---")
                    print(f"æ€»å€™é€‰æ–‡æ¡£æ•°: {response['debug']['total_candidates']}")
                    # print(f"ä½¿ç”¨çš„ä¸Šä¸‹æ–‡:\n{response['debug']['context_used'][:500]}...")
                    # print(f"å¬å›åˆ†æ•°: {response['debug']['retrieval_scores']}")
                    print("----------------")

        except KeyboardInterrupt:
            print("\næ£€æµ‹åˆ°Ctrl+Cï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            logger.log_error(e, {"stage": "main_loop_query"})
            print(f"å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}", file=sys.stderr)

    # 4. æ‰“å°æœ€ç»ˆç³»ç»ŸæŒ‡æ ‡
    status = rag_service.get_system_status()
    print("\n--- ç³»ç»Ÿæ¦‚è§ˆ ---")
    print(f"çŠ¶æ€: {status['status']}")
    print(f"æ€»æŸ¥è¯¢æ•°: {status['metrics']['total_queries']}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {status['metrics']['avg_response_time']:.2f}s")
    print(f"ç¼“å­˜å‘½ä¸­ç‡: {status['metrics']['cache_hit_rate']:.2%}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {status['metrics']['avg_confidence']:.2%}")
    print(f"æ¯åˆ†é’ŸæŸ¥è¯¢æ•°: {status['metrics']['queries_per_minute']:.2f}")
    print(f"é”™è¯¯ç‡: {status['metrics']['error_rate']:.2%}")
    print("----------------")

    # 5. å¯¼å‡ºæŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
    rag_service.export_metrics_to_file("rag_system_metrics_final.json")
    print("RAGæœåŠ¡å·²å…³é—­ã€‚")


if __name__ == "__main__":
    main()
